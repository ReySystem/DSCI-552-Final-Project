{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70f24878-03b2-4e16-98b8-3b53451c9098",
   "metadata": {},
   "source": [
    "<center><h1>Sharma_Kaustubh_Final_Project</h1></center>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb0b153-406c-40d5-97f5-34481c67e163",
   "metadata": {},
   "source": [
    "Name: Kaustubh Sharma\n",
    "<br>\n",
    "Github Username: ReySystem\n",
    "<br>\n",
    "USC ID: 1765749035"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10eb1999-f3a3-45c0-a9c2-0eca63f80eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, Flatten, Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, Embedding, Input, BatchNormalization, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04b05fb7-8c0b-4929-adc6-5b66f25f672a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the reviews\n",
    "# Loop through each file in the directory\n",
    "# Create the full file path by joining the directory path and the filename\n",
    "# Open the file for reading with UTF-8 encoding\n",
    "# Read the content of the file\n",
    "# Append a tuple containing the filename, the file content, and the assigned label to the reviews list\n",
    "# Return the list of reviews as the output of the function\n",
    "def load_reviews_from_dir_with_filenames(directory, label):\n",
    "    reviews = []\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            reviews.append((filename, file.read(), label))\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17855972-7b22-4626-9241-6fd7892ff4af",
   "metadata": {},
   "source": [
    "(b) Data Exploration and Pre-processing <br>\n",
    "i. You can use binary encoding for the sentiments , i.e y = 1 for positive sentiments and y = 0 for negative sentiments. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5939fe62-9981-4d64-b3e2-673a5852c4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 positive reviews and 1000 negative reviews.\n"
     ]
    }
   ],
   "source": [
    "# Define the directory paths for positive and negative reviews\n",
    "# Load positive reviews with label 1\n",
    "# Load negative reviews with label 0\n",
    "# Combine positive and negative reviews into a single list\n",
    "# Shuffle the combined list of reviews to mix positive and negative reviews\n",
    "# Unzip the combined list into separate lists: filenames, texts, and labels\n",
    "# Convert the labels into a NumPy array with binary values (1 for positive, 0 for negative)\n",
    "# Print the number of loaded positive and negative reviews\n",
    "positive_dir = '../Data/pos'\n",
    "negative_dir = '../Data/neg'\n",
    "positive_data = load_reviews_from_dir_with_filenames(positive_dir, 1)\n",
    "negative_data = load_reviews_from_dir_with_filenames(negative_dir, 0)\n",
    "all_data = positive_data + negative_data\n",
    "np.random.shuffle(all_data)\n",
    "filenames, texts, labels = zip(*all_data)\n",
    "labels = np.array([1 if label == 1 else 0 for label in labels])  \n",
    "print(f\"Loaded {len(positive_data)} positive reviews and {len(negative_data)} negative reviews.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ae6cc64-5d32-487d-8330-40cacef63ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample labels: [0 0 0 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Binary encoding for sentiments (already done in Cell 3)\n",
    "print(f\"Sample labels: {labels[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da288c20-b605-4819-96ec-68354725dd73",
   "metadata": {},
   "source": [
    "ii. The data are pretty clean. Remove the punctuation and numbers from the\n",
    "data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48d36ae6-568c-4c0c-9dd0-04d0963a5e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample cleaned text: ['joe versus the volcano is really one of the worse movies made in very recent memory the strangest thing is that you would think nothing would go wrong with it it has a solid cast with tom hanks meg ryan as the lead roles but you can never judge a movie by its cast if there is one good thing about joe vs the volcano it is that the plot is original unfortunately it is also incredibly stupid the movie begins with joe tom hanks going to work this opening sequence is very boring and slow it shows joe walking to his office but on the way he has to wait in a long line passing by strange and slightly depressing scenery with obnoxious lighting a sequence like this should take minutes here it takes over it is obvious that joe hates his job at his office one of his coworkers is meg ryan oddly enough she plays different roles in this movie joe leaves to go to a doctors appointment his doctor informs him that he has a brain cloud this means that in a few months he will die so what does joe do quit his job of course when he arrives home joe meets an old man named graynamore lloyd bridges graynamore tells joe that in order to get some important mineral for his company from an island the natives need someone to sacrifice to their volcano to please their fire god by a startling coincidence the boat trip to the island takes a few months by the time he reaches the island he will almost be dead from the brain cloud anyway so joe agrees graynamore gives joe a credit card to buy everything he needs to go on this great adventure joe goes on a date with meg ryan his coworker the day before the boat trip joe meets one of graynamores daughters she is also surprise meg ryan except she looks more hippyish they both have dinner and the next day joe gets driven over to his boat the lady that sails the boat is another one of graynamores daughters and wonder of wonders she is also played by meg ryan as they sail to the island meg ryan tells joe that the natives of the island have a craving for orange soda after some stupid talking scenes they also deduce that graynamore had his doctor be the one that joe went to and that the brain cloud thing was made up they do plently of cheap special effects on the boat voyage joe goes fishing and catches a hammerhead shark this is a cheap gag that has been pulled off many times but to top it off the shark is obviously rubber and fake one night there is a storm a cheaplooking lightning bolt strikes the boat and everything is cast overboard fortunately joe and meg ryan manage to find some of the luggage that joe brought to sail by pure luck they get to the island since the natives like orange soda they wear soda cans as attire stupid or what before joe leaps in the volcano he gets fed right before he jumps meg ryan pleads him not to when joe decides to she goes in with him because she loves him now this is where the movie should end but unfortunately the cheesy ending bug comes in the volcano blows the couple out and into the ocean where they land on joes luggage they float to another part of the island as they watch lava pouring out of the volcano towards the villagers i give this movie a see it only if youre a film buff that enjoys a bad movie every now and then or if you really like tom hanks or meg ryan']\n"
     ]
    }
   ],
   "source": [
    "# Define a function to clean the text\n",
    "# Remove punctuation using a regular expression\n",
    "# Remove numbers using a regular expression\n",
    "# Replace newline characters with spaces\n",
    "# Replace multiple spaces with a\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  \n",
    "    text = re.sub(r'\\d+', '', text)  \n",
    "    text = text.replace('\\n', ' ')  \n",
    "    text = re.sub(r'\\s+', ' ', text) \n",
    "    return text.strip() \n",
    "texts = [clean_text(text) for text in texts]\n",
    "print(f\"Sample cleaned text: {texts[:1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc54ab3c-2679-43a9-b322-5320dd172850",
   "metadata": {},
   "source": [
    "iii. The name of each text file starts with cv number. Use text files 0-699 in each\n",
    "class for training and 700-999 for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6581d4e9-9ab5-43cf-8284-bfb833f37778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 1400, Testing data size: 600\n"
     ]
    }
   ],
   "source": [
    "# Split the texts into training and testing sets based on the filenames\n",
    "# Include texts in the training set if the number in the filename is less than 700\n",
    "# Include labels in the training set if the number in the filename is less than 700\n",
    "# Include texts in the testing set if the number in the filename is 700 or greater\n",
    "# Include labels in the testing set if the number in the filename is 700 or greater\n",
    "# Print the sizes of the training and testing datasets\n",
    "train_texts = [text for i, text in enumerate(texts) if int(re.search(r'\\d+', filenames[i]).group()) < 700]\n",
    "train_labels = [labels[i] for i in range(len(labels)) if int(re.search(r'\\d+', filenames[i]).group()) < 700]\n",
    "test_texts = [text for i, text in enumerate(texts) if int(re.search(r'\\d+', filenames[i]).group()) >= 700]\n",
    "test_labels = [labels[i] for i in range(len(labels)) if int(re.search(r'\\d+', filenames[i]).group()) >= 700]\n",
    "print(f\"Training data size: {len(train_texts)}, Testing data size: {len(test_texts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fae8fc-ea77-4373-9992-77fb8d9cfce0",
   "metadata": {},
   "source": [
    "iv. Count the number of unique words in the whole dataset (train + test) and\n",
    "print it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "302b9c30-5222-4347-b0dd-b5ef0d957641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 47037\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Counter object to count the words\n",
    "# Loop through each text in the texts list\n",
    "# Split each text into words and update the counter with the words\n",
    "# Calculate the number of unique words by getting the length of the counter\n",
    "# Print the number of unique words\n",
    "word_counter = Counter()\n",
    "for text in texts:\n",
    "    word_counter.update(text.split())\n",
    "unique_words = len(word_counter)\n",
    "print(f\"Number of unique words: {unique_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f3f91f-96f2-4daf-a901-242fc6bd4d9d",
   "metadata": {},
   "source": [
    "v. Calculate the average review length and the standard deviation of review\n",
    "lengths. Report the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2728999-4080-4423-8350-6da0b42cb3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average review length: 644.3575\n",
      "Standard deviation of review lengths: 284.98012333099655\n"
     ]
    }
   ],
   "source": [
    "# Calculate the length of each review by counting the number of words\n",
    "# Compute the average review length\n",
    "# Compute the standard deviation of the review lengths\n",
    "# Print the average review length\n",
    "# Print the standard deviation of review lengths\n",
    "review_lengths = [len(text.split()) for text in texts]\n",
    "average_length = np.mean(review_lengths)\n",
    "std_deviation = np.std(review_lengths)\n",
    "print(f\"Average review length: {average_length}\")\n",
    "print(f\"Standard deviation of review lengths: {std_deviation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6e8878-d0a2-4ce9-a71b-e0389431deeb",
   "metadata": {},
   "source": [
    "vi. Plot the histogram of review lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca7a2ef3-504e-433e-8b4e-78ac9729ae2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot a histogram of the review lengths with 30 bins\n",
    "# Label the x-axis as 'Review Length'\n",
    "# Label the y-axis as 'Frequency'\n",
    "# Set the title of the histogram to 'Histogram of Review Lengths'\n",
    "# Display the histogram\n",
    "plt.hist(review_lengths, bins=30)\n",
    "plt.xlabel('Review Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Review Lengths')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7390a2e4-b2ae-4113-a97f-91a3f3d80358",
   "metadata": {},
   "source": [
    "vii. To represent each text (= data point), there are many ways. In NLP/Deep \n",
    "Learning terminology, this task is called tokenization. It is common to rep-\n",
    "resent text using popularity/ rank of words in text. The most common word \n",
    "in the text will be represented as 1, the second most common word will be \n",
    "represented as 2, etc. Tokenize each text document using this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7980c7ff-78e9-4837-8757-49526cf575b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review length for 70% threshold: 737\n",
      "Review length for 90% threshold: 997\n"
     ]
    }
   ],
   "source": [
    "# Sort the review lengths in ascending order\n",
    "# Calculate the review length that 70% of the reviews are below\n",
    "# Calculate the review length that 90% of the reviews are below\n",
    "# Print the review length for the 70% threshold\n",
    "# Print the review length for the 90% threshold\n",
    "sorted_lengths = sorted(review_lengths)\n",
    "L_70 = sorted_lengths[int(len(sorted_lengths) * 0.7)]\n",
    "L_90 = sorted_lengths[int(len(sorted_lengths) * 0.9)]\n",
    "print(f\"Review length for 70% threshold: {L_70}\")\n",
    "print(f\"Review length for 90% threshold: {L_90}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dd0627-b8cb-4619-972f-8f461eb82d5e",
   "metadata": {},
   "source": [
    "viii. Select a review length L that 70% of the reviews have a length below it. If\n",
    "you feel more adventurous, set the threshold to 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17dfeb03-4cd8-4e65-93b3-3b1136dedefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data: (1400, 737)\n",
      "Shape of testing data: (600, 737)\n"
     ]
    }
   ],
   "source": [
    "# Set the number of top words to consider in the vocabulary\n",
    "# Initialize the tokenizer with the specified number of top words\n",
    "# Fit the tokenizer on the texts to create the word index\n",
    "# Convert the texts to sequences of integers based on the word index\n",
    "# Set the review length L to the 70% threshold value\n",
    "# Pad and truncate the sequences to ensure all sequences have the same length L\n",
    "# Split the data into training and testing sets based on the length of the training texts\n",
    "# Assign the labels to the corresponding training and testing sets\n",
    "# Print the shape of the training data\n",
    "# Print the shape of the testing data\n",
    "top_words = 5000\n",
    "tokenizer = Tokenizer(num_words=top_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "L = L_70  \n",
    "data = pad_sequences(sequences, maxlen=L, padding='pre', truncating='post')\n",
    "x_train = data[:len(train_texts)]\n",
    "y_train = labels[:len(train_texts)]\n",
    "x_test = data[len(train_texts):]\n",
    "y_test = labels[len(train_texts):]\n",
    "print(f\"Shape of training data: {x_train.shape}\")\n",
    "print(f\"Shape of testing data: {x_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5963a84-d3a5-4148-b28d-651a73eec822",
   "metadata": {},
   "source": [
    "ix. Truncate reviews longer than L words and zero-pad reviews shorter than L\n",
    "so that all texts (= data points) are of length L."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1441177-c2b5-40f2-b76a-1d62a7bcb698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data: (1400, 737)\n",
      "Shape of testing data: (600, 737)\n"
     ]
    }
   ],
   "source": [
    "# Set the review length L to the 70% threshold value\n",
    "# Pad and truncate the sequences to ensure all sequences have the same length L\n",
    "# Split the data into training and testing sets based on the length of the training texts\n",
    "# Assign the labels to the corresponding training and testing sets\n",
    "# Print the shape of the training data\n",
    "# Print the shape of the testing data\n",
    "L = L_70  \n",
    "data = pad_sequences(sequences, maxlen=L, padding='pre', truncating='post')\n",
    "x_train = data[:len(train_texts)]\n",
    "y_train = labels[:len(train_texts)]\n",
    "x_test = data[len(train_texts):]\n",
    "y_test = labels[len(train_texts):]\n",
    "print(f\"Shape of training data: {x_train.shape}\")\n",
    "print(f\"Shape of testing data: {x_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99db5b42-3499-4c52-98b9-8f7349bf91cd",
   "metadata": {},
   "source": [
    "*****C*****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c288655b-e5a2-483e-ab5f-5d85c964a6f4",
   "metadata": {},
   "source": [
    "(c) Word Embeddings <br>\n",
    "i. One can use tokenized text as inputs to a deep neural network. However, a re-\n",
    "cent breakthrough in NLP suggests that more sophisticated representations of \n",
    "text yield better results. These sophisticated representations are called word \n",
    "embeddings. “Word embedding is a term used for representation of words \n",
    "for text analysis, typically in the form of a real-valued vector that encodes \n",
    "the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning.”4. Most deep learning modules \n",
    "(including Keras) provide a convenient way to convert positive integer rep-\n",
    "resentations of words into a word embedding by an “Embedding layer.” The \n",
    "layer accepts arguments that define the mapping of words into embeddings, \n",
    "including the maximum number of expected words also called the vocabulary \n",
    "size (e.g. the largest integer value). The layer also allows you to specify the \n",
    "dimension for each word vector, called the “output dimension.” We would like \n",
    "to use a word embedding layer for this project. Assume that we are inter-\n",
    "ested in the top 5,000 words. This means that in each integer sequence that \n",
    "represents each document, we set to zero those integers that represent words \n",
    "that are not among the top 5,000 words in the document.5 If you feel more \n",
    "adventurous, use all the words that appear in this corpus. Choose the length \n",
    "of the embedding vector for each word to be 32. Hence, each document is \n",
    "represented as a 32 × L matrix. <br>\n",
    "ii. Flatten the matrix of each document to a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28dbb77d-6da3-4133-9e47-cf0bfb345bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data: (1400, 737)\n",
      "Shape of testing data: (600, 737)\n"
     ]
    }
   ],
   "source": [
    "# Set the number of top words to consider in the vocabulary\n",
    "# Initialize the tokenizer with the specified number of top words\n",
    "# Fit the tokenizer on the texts to create the word index\n",
    "# Convert the texts to sequences of integers based on the word index\n",
    "# Set the review length L to the 70% threshold value\n",
    "# Pad and truncate the sequences to ensure all sequences have the same length L\n",
    "# Split the data into training and testing sets based on the length of the training texts\n",
    "# Assign the labels to the corresponding training and testing sets\n",
    "# Print the shape of the training data\n",
    "# Print the shape of the testing data\n",
    "top_words = 5000\n",
    "tokenizer = Tokenizer(num_words=top_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "L = L_70  \n",
    "data = pad_sequences(sequences, maxlen=L, padding='pre', truncating='post')\n",
    "x_train = data[:len(train_texts)]\n",
    "y_train = labels[:len(train_texts)]\n",
    "x_test = data[len(train_texts):]\n",
    "y_test = labels[len(train_texts):]\n",
    "print(f\"Shape of training data: {x_train.shape}\")\n",
    "print(f\"Shape of testing data: {x_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e4ec806-387f-467c-863e-9e6509a72ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kaustubh Sharma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">737</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">160,000</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23584</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m737\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m160,000\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23584\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">160,000</span> (625.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m160,000\u001b[0m (625.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">160,000</span> (625.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m160,000\u001b[0m (625.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set the embedding dimension to 32\n",
    "# Set the input length to the value of L (70% threshold length)\n",
    "# Initialize the Sequential model\n",
    "# Add an Input layer specifying the input shape explicitly\n",
    "# Add an Embedding layer with the specified number of top words, embedding dimension, and input length\n",
    "# Add a Flatten layer to flatten the output from the Embedding layer\n",
    "# Print the summary of the model to display the model architecture\n",
    "embedding_dim = 32\n",
    "input_length = L\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(input_length,)))  # Specify the input shape explicitly\n",
    "model.add(Embedding(input_dim=top_words, output_dim=embedding_dim, input_length=input_length))\n",
    "model.add(Flatten())\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee670666-a6e5-4d47-82e7-3174ee1f5a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "Shape of training embeddings: (1400, 23584)\n",
      "Shape of testing embeddings: (600, 23584)\n"
     ]
    }
   ],
   "source": [
    "# Define the input data shape with the specified input length\n",
    "# Add an Embedding layer with the specified number of top words, embedding dimension, and input length\n",
    "# Flatten the output from the Embedding layer\n",
    "# Create a model with the specified input and output layers\n",
    "# Generate embeddings for the training data using the embedding model\n",
    "# Generate embeddings for the testing data using the embedding model\n",
    "# Print the shape of the training embeddings\n",
    "# Print the shape of the testing embeddings\n",
    "input_data = Input(shape=(input_length,))\n",
    "embedding_layer = Embedding(input_dim=top_words, output_dim=embedding_dim, input_length=input_length)(input_data)\n",
    "flattened_output = Flatten()(embedding_layer)\n",
    "embedding_model = tf.keras.Model(inputs=input_data, outputs=flattened_output)\n",
    "x_train_embeddings = embedding_model.predict(x_train)\n",
    "x_test_embeddings = embedding_model.predict(x_test)\n",
    "print(f\"Shape of training embeddings: {x_train_embeddings.shape}\")\n",
    "print(f\"Shape of testing embeddings: {x_test_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a6cf7b-4fa3-43fd-8450-bc7100625730",
   "metadata": {},
   "source": [
    "*****D*****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224312ec-072c-4d92-aee0-8d8da526bf09",
   "metadata": {},
   "source": [
    "(d) Multi-Layer Perceptron <br>\n",
    "i. Train a MLP with three (dense) hidden layers each of which has 50 ReLUs \n",
    "and one output layer with a single sigmoid neuron. Use a dropout rate of \n",
    "20% for the first layer and 50% for the other layers. Use ADAM optimizer \n",
    "and binary cross entropy loss (which is equivalent to having a softmax in the \n",
    "output). To avoid overfitting, just set the number of epochs as 2. Use a batch \n",
    "size of 10. <br>\n",
    "ii. Report the train and test accuracies of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f3ee221-bf46-4d21-8c64-ff5a2287c8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.4993 - loss: 0.7128 - val_accuracy: 0.5367 - val_loss: 0.6896 - learning_rate: 0.0010\n",
      "Epoch 2/2\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5340 - loss: 0.6831 - val_accuracy: 0.5350 - val_loss: 0.6880 - learning_rate: 0.0010\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 803us/step - accuracy: 0.6264 - loss: 0.6177\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5319 - loss: 0.6896 \n",
      "Training Accuracy: 61.64%\n",
      "Testing Accuracy: 53.50%\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Sequential model for MLP\n",
    "# Add an Input layer specifying the shape based on the training embeddings\n",
    "# Add a Dense layer with 50 ReLU units\n",
    "# Add a Dropout layer with a rate of 0.2\n",
    "# Add another Dense layer with 50 ReLU units\n",
    "# Add a Dropout layer with a rate of 0.5\n",
    "# Add another Dense layer with 50 ReLU units\n",
    "# Add another Dropout layer with a rate of 0.5\n",
    "# Add a final Dense layer with a sigmoid activation for binary classification\n",
    "# Compile the model with Adam optimizer, binary cross-entropy loss, and accuracy metric\n",
    "# Define an early stopping callback to stop training when validation loss stops improving\n",
    "# Define a learning rate reduction callback to reduce the learning rate when validation loss plateaus\n",
    "# Train the MLP model with the training data and use the validation data for early stopping and learning rate reduction\n",
    "# Evaluate the model on the training data and print the training accuracy\n",
    "# Evaluate the model on the testing data and print the testing accuracy\n",
    "mlp_model = Sequential()\n",
    "mlp_model.add(Input(shape=(x_train_embeddings.shape[1],)))\n",
    "mlp_model.add(Dense(50, activation='relu'))\n",
    "mlp_model.add(Dropout(0.2))  \n",
    "mlp_model.add(Dense(50, activation='relu'))\n",
    "mlp_model.add(Dropout(0.5))  \n",
    "mlp_model.add(Dense(50, activation='relu'))\n",
    "mlp_model.add(Dropout(0.5))  \n",
    "mlp_model.add(Dense(1, activation='sigmoid'))\n",
    "mlp_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.00001)\n",
    "mlp_history = mlp_model.fit(x_train_embeddings, y_train, epochs=2, batch_size=10, validation_data=(x_test_embeddings, y_test), callbacks=[early_stopping, reduce_lr])\n",
    "train_loss, train_accuracy = mlp_model.evaluate(x_train_embeddings, y_train)\n",
    "test_loss, test_accuracy = mlp_model.evaluate(x_test_embeddings, y_test)\n",
    "print(f\"Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "print(f\"Testing Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f733d9-d48b-4db5-b514-999a29721dd0",
   "metadata": {},
   "source": [
    "(e) One-Dimensional Convolutional Neural Network: <br>\n",
    "Although CNNs are mainly used for image data, they can also be applied to text \n",
    "data, as text also has adjacency information. Keras supports one-dimensional \n",
    "convolutions and pooling by the Conv1D and MaxPooling1D classes respectively.\n",
    "i. After the embedding layer, insert a Conv1D layer. This convolutional layer \n",
    "has 32 feature maps , and each of the 32 kernels has size 3, i.e. reads embedded \n",
    "word representations 3 vector elements of the word embedding at a time. The \n",
    "convolutional layer is followed by a 1D max pooling layer with a length and \n",
    "stride of 2 that halves the size of the feature maps from the convolutional \n",
    "layer. The rest of the network is the same as the neural network above. <br>\n",
    "ii. Report the train and test accuracies of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55371732-b452-4818-80c8-462195d0b3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.4940 - loss: 0.6986 - val_accuracy: 0.5367 - val_loss: 0.6929\n",
      "Epoch 2/2\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5431 - loss: 0.6906 - val_accuracy: 0.5217 - val_loss: 0.6918\n",
      "Train Accuracy: 0.6514\n",
      "Test Accuracy: 0.5217\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Sequential model for CNN\n",
    "# Add an Embedding layer with input dimension of 5000, output dimension of 32, and input length of 737\n",
    "# Add a Conv1D layer with 32 filters, kernel size of 3, and ReLU activation\n",
    "# Add a MaxPooling1D layer with pool size of 2 and stride of 2\n",
    "# Add a Flatten layer to flatten the output from the convolutional layer\n",
    "# Add a Dense layer with 50 ReLU units\n",
    "# Add a Dropout layer with a rate of 0.2\n",
    "# Add another Dense layer with 50 ReLU units\n",
    "# Add a Dropout layer with a rate of 0.5\n",
    "# Add another Dense layer with 50 ReLU units\n",
    "# Add another Dropout layer with a rate of 0.5\n",
    "# Add a final Dense layer with a sigmoid activation for binary classification\n",
    "# Compile the model with Adam optimizer, binary cross-entropy loss, and accuracy metric\n",
    "# Train the CNN model with the training data and use the validation data for validation\n",
    "# Evaluate the model on the training data and print the training accuracy\n",
    "# Evaluate the model on the testing data and print the testing accuracy\n",
    "cnn_model = Sequential()\n",
    "cnn_model.add(Embedding(input_dim=5000, output_dim=32, input_length=737))\n",
    "cnn_model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "cnn_model.add(MaxPooling1D(pool_size=2, strides=2))\n",
    "cnn_model.add(Flatten())\n",
    "cnn_model.add(Dense(50, activation='relu'))\n",
    "cnn_model.add(Dropout(0.2))\n",
    "cnn_model.add(Dense(50, activation='relu'))\n",
    "cnn_model.add(Dropout(0.5))\n",
    "cnn_model.add(Dense(50, activation='relu'))\n",
    "cnn_model.add(Dropout(0.5))\n",
    "cnn_model.add(Dense(1, activation='sigmoid'))\n",
    "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history = cnn_model.fit(x_train, y_train, epochs=2, batch_size=10, validation_data=(x_test, y_test))\n",
    "train_loss, train_accuracy = cnn_model.evaluate(x_train, y_train, verbose=0)\n",
    "test_loss, test_accuracy = cnn_model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edff88cb-28c3-44ab-87cc-c49f1a593e5f",
   "metadata": {},
   "source": [
    "(f) Long Short-Term Memory Recurrent Neural Network: <br>\n",
    "The structure of the LSTM we are going to use is shown in the following figure. \n",
    "i. Each word is represented to LSTM as a vector of 32 elements and the LSTM\n",
    "is followed by a dense layer of 256 ReLUs. Use a dropout rate of 0.2 for both LSTM and the dense layer. Train the model using 10-50 epochs and batch \n",
    "size of 10. <br>\n",
    "ii. Report the train and test accuracies of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "016e915a-dc47-4060-ad16-b1d7515f1006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 63ms/step - accuracy: 0.4922 - loss: 0.6937 - val_accuracy: 0.6750 - val_loss: 0.6713\n",
      "Epoch 2/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 62ms/step - accuracy: 0.7124 - loss: 0.5921 - val_accuracy: 0.6500 - val_loss: 0.6217\n",
      "Epoch 3/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 63ms/step - accuracy: 0.8744 - loss: 0.3179 - val_accuracy: 0.7200 - val_loss: 0.6424\n",
      "Epoch 4/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 62ms/step - accuracy: 0.9641 - loss: 0.1087 - val_accuracy: 0.7083 - val_loss: 0.8439\n",
      "Epoch 5/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 62ms/step - accuracy: 0.9901 - loss: 0.0366 - val_accuracy: 0.7417 - val_loss: 0.8667\n",
      "Epoch 6/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 63ms/step - accuracy: 0.9954 - loss: 0.0169 - val_accuracy: 0.7067 - val_loss: 0.9206\n",
      "Epoch 7/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 64ms/step - accuracy: 0.9974 - loss: 0.0189 - val_accuracy: 0.7350 - val_loss: 1.2356\n",
      "Epoch 8/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 62ms/step - accuracy: 0.9960 - loss: 0.0140 - val_accuracy: 0.7117 - val_loss: 1.1959\n",
      "Epoch 9/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 62ms/step - accuracy: 0.9999 - loss: 0.0059 - val_accuracy: 0.7100 - val_loss: 1.4795\n",
      "Epoch 10/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 62ms/step - accuracy: 0.9911 - loss: 0.0297 - val_accuracy: 0.7333 - val_loss: 1.2537\n",
      "Train Accuracy: 0.9986\n",
      "Test Accuracy: 0.7333\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Sequential model for LSTM\n",
    "# Add an Embedding layer with input dimension of 5000, output dimension of 32, and input length of 737\n",
    "# Add an LSTM layer with 32 units and a dropout rate of 0.2\n",
    "# Add a Dense layer with 256 ReLU units\n",
    "# Add a Dropout layer with a rate of 0.2\n",
    "# Add a final Dense layer with a sigmoid activation for binary classification\n",
    "# Compile the model with Adam optimizer, binary cross-entropy loss, and accuracy metric\n",
    "# Train the LSTM model with the training data and use the validation data for validation\n",
    "# Evaluate the model on the training data and print the training accuracy\n",
    "# Evaluate the model on the testing data and print the testing accuracy\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(Embedding(input_dim=5000, output_dim=32, input_length=737))\n",
    "lstm_model.add(LSTM(32, dropout=0.2))\n",
    "lstm_model.add(Dense(256, activation='relu'))\n",
    "lstm_model.add(Dropout(0.2))\n",
    "lstm_model.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history = lstm_model.fit(x_train, y_train, epochs=10, batch_size=10, validation_data=(x_test, y_test), verbose=1)\n",
    "train_loss, train_accuracy = lstm_model.evaluate(x_train, y_train, verbose=0)\n",
    "test_loss, test_accuracy = lstm_model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d206246-235b-47c6-b25a-59290818ff22",
   "metadata": {},
   "source": [
    "References: <br>\n",
    "https://stackoverflow.com/questions/4601373/better-way-to-shuffle-two-numpy-arrays-in-unison <br>\n",
    "https://stackoverflow.com/questions/55187374/cleaning-text-with-python-and-re <br>\n",
    "https://stackoverflow.com/questions/53426276/using-regex-with-list-comprehension-in-python <br>\n",
    "https://numpy.org/doc/stable/reference/generated/numpy.std.html <br>\n",
    "https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.hist.html <br>\n",
    "https://stackoverflow.com/questions/63875821/filter-and-sort-inputed-list-of-integers-for-non-negative-numbers-python <br>\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer <br>\n",
    "https://keras.io/api/layers/core_layers/embedding/ <br>\n",
    "https://stackoverflow.com/questions/74180816/dropout-implementation-in-tf-keras <br>\n",
    "https://stackoverflow.com/questions/66016931/model-fitx-train-y-train-epochs-5-validation-data-x-test-y-test-isnt-wor <br>\n",
    "https://stackoverflow.com/questions/40888127/keras-model-compile-metrics-to-be-evaluated-by-the-model <br>\n",
    "https://stackoverflow.com/questions/43690350/using-keras-to-structure-lstm-model <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
